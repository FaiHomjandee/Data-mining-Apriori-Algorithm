{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN0RG4X4NAzQ1VtYYWIpX52",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaiHomjandee/functions/blob/main/deepclsmote_structure.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtyrCjgehNpn"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(torch.version.cuda) #10.1\n",
        "t3 = time.time()\n",
        "##############################################################################\n",
        "\"\"\"args for AE\"\"\"\n",
        "\n",
        "args = {}\n",
        "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
        "args['n_channel'] = 1#3    # number of channels in the input data\n",
        "\n",
        "args['n_z'] = 300 #600     # number of dimensions in latent space.\n",
        "\n",
        "args['sigma'] = 1.0        # variance in n_z\n",
        "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
        "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
        "args['epochs'] = 10         # how many epochs to run for (original used 200)\n",
        "args['batch_size'] = 100   # batch size for SGD\n",
        "args['save'] = True        # save weights at each epoch of training if True\n",
        "args['train'] = True       # train networks if True, else load networks from\n",
        "\n",
        "args['dataset'] = 'mnist'  #'fmnist' # specify which dataset to use\n",
        "args['num_class'] = 2\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "\n",
        "\n",
        "## create encoder model and decoder model\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "\n",
        "        # convolutional filters, work excellent with image data\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),\n",
        "            #nn.ReLU(True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            #nn.ReLU(True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            #nn.ReLU(True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "\n",
        "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),\n",
        "\n",
        "            #3d and 32 by 32\n",
        "            #nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 1, 0, bias=False),\n",
        "\n",
        "            nn.BatchNorm2d(self.dim_h * 8), # 40 X 8 = 320\n",
        "\n",
        "            nn.LeakyReLU(0.2, inplace=True) )#,\n",
        "\n",
        "        # final layer is fully connected\n",
        "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
        "    def forward(self, x, labsn):\n",
        "\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # Class images Bagging\n",
        "        num_class_in_sample = np.unique(labsn)\n",
        "        train_list = [[] for _ in range(args['num_class'])]\n",
        "        list_class_latent = [[] for _ in range(args['num_class'])]\n",
        "\n",
        "        for i, label in enumerate(labsn):\n",
        "          train_list[label].append(torch.tensor(x[i].cpu().detach().numpy()))\n",
        "\n",
        "        #list_class_latent = []\n",
        "        for i, label in enumerate(labsn):\n",
        "          #print('size each class',np.shape(train_list[k]))\n",
        "          test = torch.stack(train_list[label])\n",
        "\n",
        "          #Each class\n",
        "          x0 =  test.squeeze().to(device) # Move x0 to the same device as self.fc\n",
        "          #print('x0 aft squeeze ',x0.size())\n",
        "          x0_linear = self.fc(x0)\n",
        "          #print('x0 output ',x0_linear.size())\n",
        "          list_class_latent[label] = x0_linear\n",
        "\n",
        "        #print('list_class_latent',list_class_latent[0].size())\n",
        "\n",
        "        # Mixed Class\n",
        "\n",
        "        x = x.squeeze()\n",
        "        x = self.fc(x)\n",
        "\n",
        "        #sys.exit(1)\n",
        "\n",
        "        return x, list_class_latent\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.n_channel = args['n_channel']\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "\n",
        "        # first layer is fully connected\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.n_z, self.dim_h * 8 * 7 * 7),\n",
        "            nn.ReLU())\n",
        "\n",
        "        # deconvolutional filters, essentially inverse of convolutional filters\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4),\n",
        "            nn.BatchNorm2d(self.dim_h * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4),\n",
        "            nn.BatchNorm2d(self.dim_h * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(self.dim_h * 2, 1, 4, stride=2),\n",
        "            #nn.Sigmoid())\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print('dec')\n",
        "        #print('input ',x.size())\n",
        "        x = self.fc(x)\n",
        "        x = x.view(-1, self.dim_h * 8, 7, 7)\n",
        "        x = self.deconv(x)\n",
        "        return x\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"set models, loss functions\"\"\"\n",
        "# control which parameters are frozen / free for optimization\n",
        "def free_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def frozen_params(module: nn.Module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "\"\"\"functions to create SMOTE images\"\"\"\n",
        "\n",
        "def biased_get_class(c):\n",
        "\n",
        "    xbeg = dec_x[dec_y == c]\n",
        "    ybeg = dec_y[dec_y == c]\n",
        "\n",
        "    return xbeg, ybeg\n",
        "    #return xclass, yclass\n",
        "\n",
        "\n",
        "def G_SM(X, y,n_to_sample,cl):\n",
        "\n",
        "    # determining the number of samples to generate\n",
        "    #n_to_sample = 10\n",
        "\n",
        "    # fitting the model\n",
        "    n_neigh = 5 + 1\n",
        "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
        "    nn.fit(X)\n",
        "    dist, ind = nn.kneighbors(X)\n",
        "\n",
        "    # generating samples\n",
        "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
        "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
        "\n",
        "    X_base = X[base_indices]\n",
        "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
        "\n",
        "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
        "            X_neighbor - X_base)\n",
        "\n",
        "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
        "    return samples, [cl]*n_to_sample\n",
        "\n",
        "#xsamp, ysamp = SM(xclass,yclass)\n",
        "###############################################################################\n",
        "def calculate_diffs(z):\n",
        "    \"\"\"\n",
        "    Calculate the differences between elements in the input list 'z'\n",
        "    and return the total sum of all differences for each original element.\n",
        "\n",
        "    Args:\n",
        "        z (list): A list of lists containing numerical values.\n",
        "\n",
        "    Returns:\n",
        "        list: A list containing the total sum of differences for each original element in 'z'.\n",
        "    \"\"\"\n",
        "    all_diffs_list = []  # List to store lists of differences\n",
        "    for i, arr1 in enumerate(z):\n",
        "        diffs_for_arr1 = []  # Store differences for this arr1\n",
        "        for j, arr2 in enumerate(z):\n",
        "            if i != j:  # Skip comparing with itself\n",
        "                diffs = []  # Store diffs for this pair of arr1, arr2\n",
        "                for el1 in arr1:\n",
        "                    # Calculate the sum of Euclidean distances between el1 and all elements in arr2\n",
        "                    diff_for_el1 = sum(np.linalg.norm(np.array([el1.cpu().detach().numpy()]) - np.array([el2.cpu().detach().numpy()])) for el2 in arr2)\n",
        "                    diffs.append(diff_for_el1)  # Append diff for el1 to diffs\n",
        "                diffs_for_arr1.append(diffs)  # Append the diffs list for the current arr2\n",
        "        all_diffs_list.append(diffs_for_arr1)  # Append the diffs_for_arr1\n",
        "\n",
        "    # Sum all elements within each array to get a single total sum\n",
        "    total_sums = [sum(sum(inner_list) for inner_list in outer_list) for outer_list in all_diffs_list]\n",
        "\n",
        "    return total_sums\n",
        "\n",
        "###############################################################################\n",
        "\n"
      ]
    }
  ]
}